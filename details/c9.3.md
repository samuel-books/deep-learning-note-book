# 九、Deep Learning的常用模型或者方法

---

 
## 9.3、Restricted Boltzmann Machine (RBM)限制波尔兹曼机


假设有一个二部图，每一层的节点之间没有链接，一层是可视层，即输入数据层（v)，一层是隐藏层(h)，如果假设所有的节点都是随机二值变量节点（只能取0或者1值），同时假设全概率分布p(v,h)满足Boltzmann 分布，我们称这个模型是Restricted BoltzmannMachine (RBM)。

![](/images/1365561323_7932.jpg)

下面我们来看看为什么它是Deep Learning方法。首先，这个模型因为是二部图，所以在已知v的情况下，所有的隐藏节点之间是条件独立的（因为节点之间不存在连接），即p(h|v)=p(h1|v)…p(hn|v)。同理，在已知隐藏层h的情况下，所有的可视节点都是条件独立的。同时又由于所有的v和h满足Boltzmann 分布，因此，当输入v的时候，通过p(h|v) 可以得到隐藏层h，而得到隐藏层h之后，通过p(v|h)又能得到可视层，通过调整参数，我们就是要使得从隐藏层得到的可视层v1与原来的可视层v如果一样，那么得到的隐藏层就是可视层另外一种表达，因此隐藏层可以作为可视层输入数据的特征，所以它就是一种Deep Learning方法。

![](/images/1365561362_2427.jpg)

如何训练呢？也就是可视层节点和隐节点间的权值怎么确定呢？我们需要做一些数学分析。也就是模型了。

![](/images/1365561384_7276.jpg)

联合组态（jointconfiguration）的能量可以表示为：

![](/images/1365561400_3303.jpg)

而某个组态的联合概率分布可以通过Boltzmann 分布（和这个组态的能量）来确定：

![](/images/1365561427_1491.jpg)

因为隐藏节点之间是条件独立的（因为节点之间不存在连接），即：

![](/images/1365561464_9047.jpg)

然后我们可以比较容易（对上式进行因子分解Factorizes）得到在给定可视层v的基础上，隐层第j个节点为1或者为0的概率：

![](/images/1365561492_4178.jpg)

同理，在给定隐层h的基础上，可视层第i个节点为1或者为0的概率也可以容易得到：

![](/images/1365561523_2700.jpg)

给定一个满足独立同分布的样本集：D={v(1), v(2),…, v(N)}，我们需要学习参数θ={W,a,b}。

我们最大化以下对数似然函数（最大似然估计：对于某个概率模型，我们需要选择一个参数，让我们当前的观测样本的概率最大）：

![](/images/1365561551_7458.jpg)

也就是对最大对数似然函数求导，就可以得到L最大时对应的参数W了。


![](/images/1365561570_5666.jpg)
 

如果，我们把隐藏层的层数增加，我们可以得到Deep Boltzmann Machine(DBM)；如果我们在靠近可视层的部分使用贝叶斯信念网络（即有向图模型，当然这里依然限制层中节点之间没有链接），而在最远离可视层的部分使用Restricted Boltzmann Machine，我们可以得到DeepBelief Net（DBN）。

![](/images/1365561611_3496.jpg)

